{
    "__count__": 30,
    "data": [
        {
            "id": "1",
            "question": "What is the concept of 'Curse of Dimensionality' in Machine Learning?",
            "choices": [
                "A. It refers to the difficulty in visualizing high-dimensional data.",
                "B. It refers to the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset.",
                "C. It refers to the computational complexity of mathematical operations in high-dimensional spaces.",
                "D. It refers to the overfitting that occurs when the model is too complex."
            ],
            "explanation": "The 'Curse of Dimensionality' refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.",
            "answer": "B",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "2",
            "question": "What is the difference between 'one-hot encoding' and 'label encoding'?",
            "choices": [
                "A. 'One-hot encoding' is used for ordinal variables, while 'label encoding' is used for nominal variables.",
                "B. 'Label encoding' is used for ordinal variables, while 'one-hot encoding' is used for nominal variables.",
                "C. Both 'one-hot encoding' and 'label encoding' are used for ordinal variables.",
                "D. Both 'one-hot encoding' and 'label encoding' are used for nominal variables."
            ],
            "explanation": "'Label encoding' and 'one-hot encoding' are two common methods used to convert categorical data into numerical data. 'Label encoding' assigns each unique category in a categorical variable with an integer. No new columns are created. On the other hand, 'one-hot encoding' creates new (binary) columns, indicating the presence of each possible value from the original data.",
            "answer": "B",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "3",
            "question": "What is 'Early Stopping' in model training?",
            "choices": [
                "A. It's a form of regularization used to avoid overfitting when training a learner with an iterative method.",
                "B. It's a technique used to speed up training by stopping the training process before it completes all iterations.",
                "C. It's a method used to stop training when the model's performance starts to decrease.",
                "D. It's a technique used to stop training when the model's performance does not improve on an held-out validation set."
            ],
            "explanation": "'Early stopping' is a form of regularization used to avoid overfitting when training a machine learning model with an iterative method, such as gradient descent. Such methods update the model's parameters (such as the weights in a neural network) iteratively, and early stopping halts this iterative process when the model's performance stops improving on a held-out validation dataset.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "4",
            "question": "What is the difference between 'Random Forest' and 'Gradient Boosting' algorithms?",
            "choices": [
                "A. 'Random Forest' is a bagging algorithm, while 'Gradient Boosting' is a boosting algorithm.",
                "B. 'Gradient Boosting' is a bagging algorithm, while 'Random Forest' is a boosting algorithm.",
                "C. Both 'Random Forest' and 'Gradient Boosting' are bagging algorithms.",
                "D. Both 'Random Forest' and 'Gradient Boosting' are boosting algorithms."
            ],
            "explanation": "'Random Forest' is a bagging algorithm and 'Gradient Boosting' is a boosting algorithm. Both are ensemble methods, but they combine the models in different ways. In 'Random Forest', each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In 'Gradient Boosting', each new tree is fit on a modified version of the original data set.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "5",
            "question": "What is 'Transfer Learning' in the context of Machine Learning?",
            "choices": [
                "A. It's a technique where a pre-trained model is used on a new problem.",
                "B. It's a technique to transfer knowledge from one model to another.",
                "C. It's a technique to transfer knowledge from one problem domain to another.",
                "D. All of the above."
            ],
            "explanation": "'Transfer Learning' is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.",
            "answer": "D",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "6",
            "question": "What is the purpose of 'ReLU' in a neural network?",
            "choices": [
                "A. To introduce non-linearity in the neural network.",
                "B. To normalize the output of the neural network.",
                "C. To speed up the training process of the neural network.",
                "D. To prevent overfitting in the neural network."
            ],
            "explanation": "'ReLU' stands for 'Rectified Linear Unit'. It is the most commonly used activation function in neural networks and deep learning models. The function returns 0 if it receives any negative input, but for any positive value 'x' it returns that value back. It's used to introduce non-linearity in the neural network.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "7",
            "question": "What is 'Batch Normalization' in the context of Neural Networks?",
            "choices": [
                "A. It's a technique to provide any layer in a neural network with inputs that are zero mean/unit variance.",
                "B. It's a technique to normalize the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.",
                "C. It's a technique to make the weights of a neural network have zero mean and unit variance.",
                "D. Both A and B."
            ],
            "explanation": "'Batch Normalization' is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance, and it is a technique to normalize the output of the previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This technique helps to speed up learning in deep neural networks by reducing internal covariate shift, and it has become a standard component of most state-of-the-art neural networks.",
            "answer": "D",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "8",
            "question": "What is the purpose of 'L1' and 'L2' regularization?",
            "choices": [
                "A. They are techniques to prevent overfitting in a machine learning model.",
                "B. They are techniques to increase the speed of training a machine learning model.",
                "C. They are techniques to increase the complexity of a machine learning model.",
                "D. They are techniques to decrease the complexity of a machine learning model."
            ],
            "explanation": "'L1' and 'L2' are regularization techniques used to prevent overfitting in a machine learning model by adding a penalty term to the loss function. The penalty term encourages the model to have smaller weights, which makes the model simpler and thus less likely to overfit.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "9",
            "question": "What is 'Ensemble Learning' in the context of Machine Learning?",
            "choices": [
                "A. It's a technique where multiple models are trained to solve the same problem and combined to get better results.",
                "B. It's a technique where one model is trained to solve multiple problems.",
                "C. It's a technique where the model is trained on an ensemble of different datasets.",
                "D. It's a technique where the model is trained multiple times on the same dataset."
            ],
            "explanation": "'Ensemble Learning' is a machine learning paradigm where multiple models (often called 'weak learners') are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "10",
            "question": "What is the difference between 'Ridge' and 'Lasso' regression?",
            "choices": [
                "A. 'Ridge' regression uses L1 regularization while 'Lasso' regression uses L2 regularization.",
                "B. 'Lasso' regression uses L1 regularization while 'Ridge' regression uses L2 regularization.",
                "C. Both 'Ridge' and 'Lasso' regression use L1 regularization.",
                "D. Both 'Ridge' and 'Lasso' regression use L2 regularization."
            ],
            "explanation": "'Ridge' and 'Lasso' regression are two types of linear regression models that use different types of regularization. 'Ridge' regression uses L2 regularization, which adds a penalty equal to the square of the magnitude of coefficients. On the other hand, 'Lasso' regression uses L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients.",
            "answer": "B",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "11",
            "question": "What is 'Data Augmentation' in the context of Machine Learning?",
            "choices": [
                "A. It's a technique to artificially create new training data from existing training data.",
                "B. It's a technique to increase the size of the dataset by collecting more data.",
                "C. It's a technique to clean the training data.",
                "D. It's a technique to reduce the size of the dataset."
            ],
            "explanation": "'Data Augmentation' is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "12",
            "question": "What is the purpose of 'Max Pooling' in a Convolutional Neural Network (CNN)?",
            "choices": [
                "A. To reduce the spatial dimensions of the output volume.",
                "B. To increase the spatial dimensions of the output volume.",
                "C. To normalize the output of the previous activation layer.",
                "D. To introduce non-linearity in the neural network."
            ],
            "explanation": "'Max Pooling' is a pooling operation that is typically added to CNNs following individual convolutional layers. When added to a model, max pooling reduces the dimensionality of images by reducing the number of pixels in the output from the previous convolutional layer.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "13",
            "question": "What is the difference between 'Batch Gradient Descent' and 'Mini-Batch Gradient Descent'?",
            "choices": [
                "A. 'Batch Gradient Descent' uses the entire training set to compute the gradient of the cost function, while 'Mini-Batch Gradient Descent' uses a subset of the training set.",
                "B. 'Mini-Batch Gradient Descent' uses the entire training set to compute the gradient of the cost function, while 'Batch Gradient Descent' uses a subset of the training set.",
                "C. Both 'Batch Gradient Descent' and 'Mini-Batch Gradient Descent' use the entire training set to compute the gradient of the cost function.",
                "D. Both 'Batch Gradient Descent' and 'Mini-Batch Gradient Descent' use a subset of the training set to compute the gradient of the cost function."
            ],
            "explanation": "'Batch Gradient Descent' uses the entire training set to compute the gradient of the cost function, while 'Mini-Batch Gradient Descent' uses a subset of the training set. With 'Mini-Batch Gradient Descent', you can replace the actual gradient (calculated from the entire data set) with an estimate of the gradient (calculated from a randomly selected subset of the data). Especially in big data applications, this can help to speed up gradient-based optimization algorithms significantly.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "14",
            "question": "What is 'Principal Component Analysis' (PCA) used for?",
            "choices": [
                "A. PCA is used to compress the data by reducing the number of dimensions.",
                "B. PCA is used to decompress the data by increasing the number of dimensions.",
                "C. PCA is used to classify the data into different categories.",
                "D. PCA is used to cluster the data into different groups."
            ],
            "explanation": "'Principal Component Analysis' (PCA) is a dimensionality reduction technique that is commonly used in machine learning and data visualization. It can be thought of as a projection method where data with 'm' columns (features) is projected into a subspace with 'm' or fewer columns, whilst retaining the essence of the original data.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "15",
            "question": "What is the purpose of 'Word Embeddings' in Natural Language Processing (NLP)?",
            "choices": [
                "A. To map words or phrases from the vocabulary to vectors of real numbers.",
                "B. To map words or phrases from the vocabulary to a dictionary of words.",
                "C. To convert the words in the vocabulary to lower case.",
                "D. To remove stop words from the vocabulary."
            ],
            "explanation": "'Word Embeddings' are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "16",
            "question": "What is the difference between 'Long Short Term Memory' (LSTM) and 'Gated Recurrent Unit' (GRU)?",
            "choices": [
                "A. LSTM has three gates (input, output, forget) while GRU has two gates (reset, update).",
                "B. GRU has three gates (input, output, forget) while LSTM has two gates (reset, update).",
                "C. Both LSTM and GRU have three gates (input, output, forget).",
                "D. Both LSTM and GRU have two gates (reset, update)."
            ],
            "explanation": "Both LSTM (Long Short Term Memory) and GRU (Gated Recurrent Unit) are types of recurrent neural network (RNN) architecture used in deep learning. The key difference between them is that LSTM has three gates (input, output, forget), while GRU has two gates (reset, update). This makes GRUs a simpler and more efficient model for certain tasks.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "17",
            "question": "What is 'Autoencoder' in the context of Machine Learning?",
            "choices": [
                "A. It's a type of artificial neural network used for learning efficient codings of input data.",
                "B. It's a type of artificial neural network used for generating new data that is similar to the input data.",
                "C. It's a type of artificial neural network used for classifying input data into different categories.",
                "D. It's a type of artificial neural network used for clustering input data into different groups."
            ],
            "explanation": "An 'Autoencoder' is a type of artificial neural network used for learning efficient codings of input data. It's typically used for the purpose of dimensionality reduction and feature learning.",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "18",
            "question": "What is the purpose of 'Attention Mechanism' in the context of Machine Learning?",
            "choices": [
                "A. It's used to focus on certain parts of the input data that are more relevant to the task at hand.",
                "B. It's used to pay equal attention to all parts of the input data.",
                "C. It's used to ignore certain parts of the input data that are not relevant to the task at hand.",
                "D. Both A and C."
            ],
            "explanation": "The 'Attention Mechanism' is a technique used in machine learning models, especially in deep learning models, to focus on certain parts of the input data that are more relevant to the task at hand, and to ignore other parts. It's particularly useful in tasks such as machine translation, where it's important to focus on the right words in the input sequence when generating the output sequence.",
            "answer": "D",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "19",
            "question": "What is 'Reinforcement Learning' in the context of Machine Learning?",
            "choices": [
                "A. It's a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward.",
                "B. It's a type of machine learning where an agent learns to make decisions based on a fixed set of rules.",
                "C. It's a type of machine learning where an agent learns to make decisions based on a predefined set of actions.",
                "D. It's a type of machine learning where an agent learns to make decisions based on the actions taken by other agents."
            ],
            "explanation": "'Reinforcement Learning' is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. The agent learns from the consequences of its actions, rather than from being explicitly taught and it selects its actions on basis of its past experiences (exploitation) and also by new choices (exploration).",
            "answer": "A",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "20",
            "question": "What is 'Generative Adversarial Network' (GAN) in the context of Machine Learning?",
            "choices": [
                "A. It's a class of machine learning systems invented by Ian Goodfellow and his colleagues in 2014.",
                "B. It's a class of machine learning systems where two neural networks contest with each other in a game.",
                "C. It's a class of machine learning systems where one neural network, called the generator, generates new data instances, while the other, the discriminator, evaluates them for authenticity.",
                "D. All of the above."
            ],
            "explanation": "'Generative Adversarial Network' (GAN) is a class of machine learning systems invented by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.",
            "answer": "D",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "21",
            "question": "Write a Python function to implement a basic 'K-Nearest Neighbors' (KNN) model.",
            "explanation": "The function should take a dataset and a value for 'K' as arguments and return a trained KNN model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "22",
            "question": "Write a Python function to implement a basic 'Naive Bayes' model.",
            "explanation": "The function should take a dataset as an argument and return a trained Naive Bayes model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "23",
            "question": "Write a Python function to implement a basic 'Random Forest' model.",
            "explanation": "The function should take a dataset as an argument and return a trained Random Forest model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "24",
            "question": "Write a Python function to implement a basic 'Gradient Boosting' model.",
            "explanation": "The function should take a dataset as an argument and return a trained Gradient Boosting model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "25",
            "question": "Write a Python function to implement a basic 'Deep Neural Network' (DNN) model.",
            "explanation": "The function should take a dataset as an argument and return a trained DNN model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "26",
            "question": "Write a Python function to implement a basic 'Convolutional Neural Network' (CNN) model.",
            "explanation": "The function should take a dataset as an argument and return a trained CNN model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "27",
            "question": "Write a Python function to implement a basic 'Decision Tree' model.",
            "explanation": "The function should take a dataset as an argument and return a trained decision tree model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "28",
            "question": "Write a Python function to implement a basic 'Support Vector Machine' (SVM) model.",
            "explanation": "The function should take a dataset as an argument and return a trained SVM model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "29",
            "question": "Write a Python function to implement a basic 'Linear Regression' model.",
            "explanation": "The function should take a dataset as an argument and return a trained linear regression model.",
            "level": "Senior",
            "domain": "AI"
        },
        {
            "id": "30",
            "question": "Write a Python function to implement a basic 'Logistic Regression' model.",
            "explanation": "The function should take a dataset as an argument and return a trained logistic regression model.",
            "level": "Senior",
            "domain": "AI"
        }
    ]
}